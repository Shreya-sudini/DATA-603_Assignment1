# DATA-603_Assignment1
# BIG DATA
Big data is huge amount of data that is very complex to process and analyze and cannot be analyzed by regular system, it needs additional processing tools to effectively process, analyse and derive meaningful insights.

## Examples
These days big data is generated everywhere.
+ Healthcare - The Electronic health records(EHRs) that contain patient data, medical history, lab results and provide better treatment plans.
+ Social media data - The data generated from various applications like Twitter, Instagram, Snapchats in the form of text, images, videos, audios.
+ Marketing - Data taken recorded from the customer is analyzed for behavior and helps in finding emerging trends in the market.
+ Business - Big data analytics can be used to track revenue information, employee satisfaction, customer demands to run management effectively.
+ Cybersecurity - Data from the unusual behavior, online traffic is recorded and can be used for preventing cyber attacks in the future.

## Types 
![image](https://github.com/Shreya-sudini/DATA-603_Assignment1/assets/86612479/6526c58b-b810-4848-8467-e18a08353924)

+ Structured Data - The data is well organized into structures such as databases, tables and is easy to understand, hence, it is easy to handle. It can be accessed through simple query languages as they are in tables and databases.
+ Unstructured Data - This kind of data has no definite structure. This data is difficult to handle and analyze. Social media texts, tweets, images, videos, audios come under unstructured data.
+ Semi-structured Data - Semi structured data is combination of structured and unstructured data. This means that the data may not be definitely organized but it still has some metadata to it that can be understood. XML files, JSON files, NoSQL texts, CSV files come under semi sructured.
+ Giospatial Data - This data provides information on things thta are located on or close to earth. It gives location information such as the coordinates of particular place and attribute information such as occurance of natural disaster.
+ Machine or operational logging data - This kind of data is generated by machine without human intervention. Application log files and call detail records come under this.
+ Open-source data - The handle the crucial data which is within the organization auhtority. The user can create a system of their demands and needs. Google Public Data Explorer is an example of this type.

## 6V's of Big Data
Big data can be characterized by 6V's which are volume, velocity, veracity, variability, variety, value.

+ ### Volume
  It is the amount of data that is generated that is too difficult to process and analyze. Big companies like Walmart and Amazon get hundreds of orders per minute, which they have to keep record of.
+ ### Velocity
  The speed at which the data is generated. Some data such as social media data, streaming data come at high velocities.
+ ### Variety
  There is variety of data of coming in. Big data can come as structured, unstructured and semi-structured data, giospatial, etc. For finding new trends the system needs to analyze the past records which might be in database and also take the text, image files from social media for analysis.
+ ### Veracity
  It is the quality and accuracy of the data.Considering the volume and variety of the data, there can be some of which is not useful. To analyze and process the data it needs to be cleaned and in good quality to extract good value out of it. 
+ ### Variability
  As the data is generated at high velocity and sometimes at slow velocities. Also, the shape and structure of data can change with the time. The data is variable, which can create inconsistencies.
+ ### Value
  The end goal of big data analytics is to extract value for a purpose. Values refers to usefulness of the data to achieve a goal after subjecting the data to analysis.

## Phases of Big Data Analysis
There are 5 phases of Big Data Analysis:

![image](https://github.com/Shreya-sudini/DATA-603_Assignment1/assets/86612479/fc9f97d4-6aa2-42e8-806a-2f5d06f39640)

+ ### Phase 1: Data Acquisition and Recording
  There is ton of data generated from many sources such as sensors, social media, satellites, etc, from which by applying filters we can collect the necessary data for processing. Metadata can be generated on how and what data was recorded . For example, the sensor records the data along with the timestamp and the date is was generated.
+ ### Phase 2: Information extraction and cleaning
  Obtaining required information from the sources and make it suitable for analysis by providing it in a structured manner. Also, the data must be identified for missing values, inconsistencies, outliers and noise and clearing them out so they do not effect the analysis.
+ ### Phase 3: Data integration, Aggregation and Representation
  The obtained data from various sources should be combined, this can help in reducing the inconcsistencies and redundancies. The data obtained can be represented in the form of graphs and charts using visualization techniques.
+ ### Phase 4: Query Processing, Data Modelling and Analysis
  In this phase, analysis is done on the data obtained. Queries are given to gain the data subsets from the overall data for effective analysis. Data models are used to organize the data elements into a structure and to identify the relationships among them. Various analysis methods are used for processing and gaining value, as big data required more than SQL querying.
+ ### Phase 5: Interpretation
  After the analysis, now it's time for understanding the results. At this phase, the metadata recorded at the time of acquisition and visualizations will help for understanding and making decisions.

## Challenges in Big Data Analysis
There are several challenges that come with the analysis of such huge amount of data.

+ ### Heterogenity and Incompleteness
  The machine algorithms expect to receive homogeneous data which is realistically not possible as the real life data comes in different types. Even after cleaning the data there still might be some kind of inconsistencies such as missing values, different kind of values which would effect the analysis.
+ ### Scale
  As mentioned there is huge volume of data generated but the computing resources are less, which can be problematic as the analysis and predictions would slow down. The storage can be upgraded to cloud computing to effectively store and parallel computing can help in processing faster.
+ ### Timeliness
  In most cases the results are required immediately, but the query processing takes time on large datasets. Having indexes and getting new index structures can help resolving the time processing issues.
+ ### Privacy
+ Provacy becomes a concern with big data as it needs to access a lot of data of a person to make proper conclusions. For example, a doctor or healthcare facility needs to record every bit of information such medication, medical records of patient which some may not be comfortable in sharing with.
+ ### Human Collaboration
  Big data analytics system has humans and machine collaborating together for best results, but human collaboration becomes a challenge in case of false reviews, uncertain inputs, etc.

## References
+ What is big data? Definition and best practices. (2022, August 8). Spiceworks. https://www.spiceworks.com/tech/big-data/articles/what-is-big-data/amp/
+ Rice, M. (2023). 25 Big Data examples and applications. Built In. https://builtin.com/big-data/big-data-examples-applications
+ Schaafsma, S. (2023). Big data: The 6 vs you need to look at for important insights. Motivaction. https://www.motivaction.nl/en/actualities/news/big-data-the-6-vs-you-need-to-look-at-for-important-insights
+ Zhao, C. (n.d.). Five Phases in the Big Data Processing Pipeline (notes). www.linkedin.com. https://www.linkedin.com/pulse/five-phases-big-data-processing-pipeline-notes-christiane-zhao
  
  
  
